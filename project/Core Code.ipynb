{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Polarity and lexical complexity to study social topics in music genres\n",
    "\n",
    "## Goals\n",
    "\n",
    "This work aims at conducting a sentiment analysis through music genres. There are many ways to complete this goal, however it is important that the parameters and analysers we use are fitting our data. Sentiment analysis is a very large field, and we believe that we used the librairies and functions that are the most appropriate for the chosen dataset. \n",
    "This work can be divided into 4 parts:\n",
    " 1. Data imports: structures, sorting and wrangling\n",
    " 2. Classifiers: Choosing the methods of analysis and extracting features\n",
    " 3. Visualization: How to meaningfully represent the data\n",
    " 4. Finalization: Organization of the outputs\n",
    " \n",
    " \n",
    "##### Dataset credits:\n",
    "musiXmatch dataset, the official lyrics collection for the Million Song Dataset, \n",
    "available at: http://labrosa.ee.columbia.edu/millionsong/musixmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import gensim as gs\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# internal imports\n",
    "import helpers as HP\n",
    "\n",
    "\n",
    "\n",
    "# Constants: PS! put in your own paths to the files\n",
    "GLOVE_FOLDER = 'glove.twitter.27B'\n",
    "GS_FOLDER = 'gensim_glove_twitter_27B/'\n",
    "GS_25DIM = GS_FOLDER + \"gensim_glove_25dim.txt\"\n",
    "GS_50DIM = GS_FOLDER + \"gensim_glove_50dim.txt\"\n",
    "GS_100DIM = GS_FOLDER + \"gensim_glove_100dim.txt\"\n",
    "GS_200DIM = GS_FOLDER + \"gensim_glove_200dim.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data imports: structures, sorting and wrangling\n",
    "- Import the Songs Titles and Artists from MusiXMatch  779052 songs  \n",
    "- Import the Year from million song additional files   498466 songs\n",
    "- Import the Genres from TagTraum  255015 songs    \n",
    "- Import the lyrics & the bag of words 91625 songs\n",
    "\n",
    "\n",
    "## 1.1. Importing the songs\n",
    "For now we only use titles and artist names, we are able to handle this part with only the musixmatch website. We download the data and put it into a dataframe with the Id of MusiXMatch(MXM_Tid) and the Track ID of the Million Song DataSet(Tid). Because we might have data that is given with one classification or the other, we decide to keep the two IDs, but we are fully aware that having two IDs is not giving additional information, it is only to be sure that other datasets will be easier to merge.   \n",
    "We for now, we get 779052 song's artists and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the text file in a DataFrame, removing exceptions (incomplete data)\n",
    "matches = pd.read_table('Data/mxm_779k_matches.txt', error_bad_lines=False)\n",
    "\n",
    "#Changing the column's title in order to be clearer\n",
    "matches.columns = ['Raw']\n",
    "\n",
    "#Getting the Tid\n",
    "matches['Tid'] = matches['Raw'].str.split('<SEP>', expand=True)[0]\n",
    "\n",
    "#Extracting artist names\n",
    "matches['Artist_Name'] = matches['Raw'].str.split('<SEP>', expand=True)[1]\n",
    "\n",
    "#Extracting titles\n",
    "matches['Title'] = matches['Raw'].str.split('<SEP>', expand=True)[2]\n",
    "\n",
    "#Extractign MXM_Tid\n",
    "matches['MXM_Tid'] = matches['Raw'].str.split('<SEP>', expand=True)[3]\n",
    "\n",
    "#Dropping rows we do not need ()\n",
    "matches = matches.drop(matches.index[:17])\n",
    "\n",
    "#Droppign the column with raw data\n",
    "matches = matches.drop('Raw', axis=1)\n",
    "\n",
    "#set index Track ID\n",
    "matches.set_index('Tid',inplace=True)\n",
    "\n",
    "#Displaying results\n",
    "display(matches.shape)\n",
    "display(matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remarks:\n",
    "- There are two distinct identifiers for the same data. Because we might have data that is given with one classification or the other, we decide to keep the two IDs, but we are fully aware that having two IDs is not giving additional information, it is only to be sure that other datasets will be easier to merge.\n",
    "- This is only containing the artist and title, we need further informations such as the genre and the bags of words for each song. \n",
    "\n",
    "## 1.2. Extracting the Year of the songs\n",
    "\n",
    "We download the text file from the \"A million song\" website. It is provided as an additional feature of the dataset.  \n",
    "We merge the year dataset with the artists and song titles in the same dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the year of publication data, skipping incomplete data in order to avoid errors\n",
    "years = pd.read_table('Data/tracks_per_year.txt', error_bad_lines=False)\n",
    "#Changing the column's title in order to be clearer\n",
    "years.columns = ['Raw']\n",
    "\n",
    "#Getting the year publication\n",
    "years['year'] = years['Raw'].str.split('<SEP>', expand=True)[0]\n",
    "\n",
    "#Getting the Tid\n",
    "years['Tid'] = years['Raw'].str.split('<SEP>', expand=True)[1]\n",
    "\n",
    "#Dropping the raw data\n",
    "years = years.drop('Raw', axis=1)\n",
    "\n",
    "#set index Track ID\n",
    "years.set_index('Tid',inplace=True)\n",
    "\n",
    "#Appending the years to the original DataFrame\n",
    "matches = pd.merge(matches, years, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the results\n",
    "print(matches.shape)\n",
    "display(matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks:  \n",
    "We delete the rows without year infos. Thus why the dataframe contains less songs. In order to be able to be as complete as accurate as possible, we consider only full matching.\n",
    "\n",
    "## 1.3 Importing genres\n",
    "We will now append each genre to a specific track.  \n",
    "We download the data from the TagTraum dataset and merge them without our previous dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a DataFrame to store the genres:\n",
    "GenreFrame = pd.read_table('Data/msd-topMAGD-genreAssignment.txt', names=['Tid', 'genre'])\n",
    "\n",
    "#set index Track ID\n",
    "GenreFrame.set_index('Tid',inplace=True)\n",
    "\n",
    "#merge the new datas with the previous dataframe\n",
    "matches = pd.merge(GenreFrame, matches, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "print(matches.shape)\n",
    "display(matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment:\n",
    "The dataframe contains once again less songs. We proceed this way for the same reason as mentioned in the part before.\n",
    "\n",
    "## 1.4. Importing Location\n",
    "\n",
    "We download the file with the location of every artist from the additional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a DataFrame to store the location:\n",
    "location = pd.read_csv('Data/artist_location.txt', sep=\"<SEP>\",header=None,names=['ArtistID','Latitude','Longitude','Artist_Name','City'])\n",
    "#Keep useful datas\n",
    "location.drop(['ArtistID','City'],inplace=True,axis=1)\n",
    " \n",
    "#matches = pd.merge(location, matches, on='Tid')\n",
    "matches.reset_index(inplace=True)\n",
    "matches = pd.merge(location, matches, on='Artist_Name')\n",
    "matches.set_index('Tid',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying results\n",
    "display(matches.head())\n",
    "print(matches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Bags of words, extracting them from the train dataset\n",
    "\n",
    "We downloaded the train datafile which is 30% of the whole dataset.\n",
    "Inside we have a list of the 5000 words the most used in the ... songs.\n",
    "We then make two dataframes: \n",
    "- One with the Id of every songs and their lyrics. We merge this with our previous dataframe.\n",
    "     \n",
    "     The lyrics are presented as follow : [(id of word),(occurence in song)][2,24][5,47]...  \n",
    "\n",
    "\n",
    "- Another one with the 5000 top words of the songs (Bag of Words)           \n",
    "            \n",
    "  \n",
    "We work with only 30% of the whole dataset because we use the MusicXMatch dataset and it is the only data that are available.  \n",
    "The rest of the data are not free. You could see that page : https://developer.musixmatch.com/plans to verify.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import file\n",
    "lyrics = pd.read_table('Data/mxm_dataset_train.txt', error_bad_lines=False)\n",
    "\n",
    "#change name of the column\n",
    "lyrics.columns = ['Raw_Training']\n",
    "\n",
    "# take the bag of word to use it later\n",
    "words_train = lyrics.iloc[16]\n",
    "\n",
    "#drop useless rows\n",
    "lyrics=lyrics[17:].copy()\n",
    "\n",
    "# get TrackID, MxMID and lyrics and put them separated columns\n",
    "def sortdata(x):\n",
    "    splitted = x['Raw_Training'].split(',')\n",
    "    x['Tid']=splitted[0]\n",
    "    #x['MXM_Tid']=splitted[1]\n",
    "    x['words_freq']=splitted[2:]\n",
    "    return x\n",
    "\n",
    "#Apply the function to every column\n",
    "lyrics = lyrics.apply(sortdata,axis=1)\n",
    "lyrics = lyrics[['Tid','words_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set index Track ID\n",
    "lyrics.set_index('Tid',inplace=True)\n",
    "\n",
    "#Appending the years to the original DataFrame\n",
    "matches = pd.merge(matches, lyrics, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the results\n",
    "print(matches.shape)\n",
    "display(matches.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments on the size:\n",
    "Due to the fact that we do not have access to the entire dataset, our analysis is limited to the 30% that is freely available on MusixMatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. From generic bags of words to lyrics\n",
    "\n",
    "\n",
    "We Create a function that take the list of the word and the occurency in one song : [(id of word),(occurency in the song)][2,24][5,47]...  \n",
    "And output all the corresponding words in a list  \n",
    "\n",
    "For example : [1:2,2:5,3:3] gives us --> [i,i,the,the,the,the,the,you,you,you]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the datas\n",
    "bag_of_words = words_train\n",
    "# clean the data and split it to create a list of 5000 words\n",
    "bag_of_words = bag_of_words.str.replace('%','')\n",
    "bag_of_words = bag_of_words.str.split(',')\n",
    "\n",
    "display(bag_of_words.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining a function\n",
    "def create_text(words_freq):\n",
    "    #create the final list of all words\n",
    "    list_words=''\n",
    "    #iterate over every id of words\n",
    "    for compteur in words_freq:\n",
    "        \n",
    "        word = bag_of_words[0][int(compteur.split(':')[0])-1]\n",
    "        times = int(compteur.split(':')[1])\n",
    "        \n",
    "        #Separating every word with a space to be able to work on it with librairies during part 2\n",
    "        for i in range(times):\n",
    "            list_words += ' ' + word + ' '\n",
    "    return list_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the function\n",
    "print(create_text(lyrics.iloc[0]['words_freq']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments on part one:\n",
    "\n",
    "As it is noticeable through each step, we loose data every time we merge datasets. We chose this approach because we only want to deal with complete information in order to be coherent. We want to compare parameters between items and we believe that the analysis is less relevant if we consider a larger dataset that contains data incomplete data.\n",
    "\n",
    "We now have 38 513 songs, but for each one we have all the features that we want to use. We will analyse our data with different parameters, thus why it is important that it each song provides each item. Later in the analysis we may use data from 1.4. (providing 103 401 songs) in order to get a broader overview. \n",
    "\n",
    "\n",
    "# 2. Classifiers: Choosing the methods of analysis and extracting features\n",
    "In order to analyse songs, we will use sentiment analysis on the lyrics. We chose to use 2 key features, which are the polarity and the lexical complexity. Because we only use bags of words, some parameters such as rhymes and structures are not defined with bags of words and they should be taken in consideration when speaking of the whole complexity of lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Word polarity\n",
    "##### Vader package\n",
    "VADER, which stands for Valence Aware Dictionary and sEntiment Reasoner, is a sentiment analysis package that provides a polarity score for a given word or sentences. It is known to be a very powerful tool, especially because it was trained on tweets, meaning that it takes into account most of modern vocabulary. This is especially relevant for our project because we deal with modern music, implying that the words that are used are as modern as the ones analysed by VADER on tweets. The fact that the sentiment analyser takes its roots from the same vocabulary is make the analysis more relevant.\n",
    "\n",
    "\n",
    "Polarity is expressed between -1 (negative polarity) and 1 (positive polarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment.sentiment_analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the analyser\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Lyrics' complexity\n",
    "Because we want to be able to know what type of audience a specific type of music is targeting we need to analyse the complexity of the lyrics. We are aware that dividing an audience into social profiles is far beyond the scope of our analysis. We do not have enough sociological knowledge to categorize an audience in a precise way. This is the reason why we will use large indicators. We want to know how complex a set of word is, and the only social assumption we will make is that complexity is correlated with the age and the educational level of the audience. \n",
    "\n",
    "We use the occurence of each word in the whole dataset.\n",
    "\n",
    "#####  Extracting the vocabulary  \n",
    "Importing the most used words and their count inside the dataset in order to start a text processing analysis.\n",
    "##### Extracting additional features\n",
    "From the dataset, there was some given metadata. The total word count is 55 163 335.\n",
    "\n",
    "Because of the long tail effect of language, we will proceed with the first 10 000 words of the list. This will enable us to have less computing time when iterating on the full_word_list.\n",
    "\n",
    "From the vocabulary we remove stopwords. Those are too often mentionned in every level of language to be relevant for this analysis.\n",
    "\n",
    "We then compute the percentage of occurence, because it will help us when dealing with lyrics' complexity.\n",
    "We then use the occurence precentage to get a Complexity weight. It means that when a word is used a lot it will have a low weight and a high weight for words rarely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Word_count_total = 55163335\n",
    "\n",
    "#Importing the data, putting it in a DataFrame\n",
    "full_word_list = pd.read_table('Data/full_word_list.txt')\n",
    "#Renaming the columns\n",
    "full_word_list.columns = ['Word']\n",
    "#Extracting word count\n",
    "full_word_list['Count'] = pd.to_numeric(full_word_list['Word'].str.split('<SEP>', expand=True)[1])\n",
    "#Extracted words that were used\n",
    "full_word_list['Word'] = full_word_list['Word'].str.split('<SEP>', expand=True)[0]\n",
    "#Dropping rows we will not use\n",
    "full_word_list = full_word_list.drop(full_word_list.index[:6])\n",
    "\n",
    "#Extracting the first 50 0000  values, because the rest is not stemmed and not necessarly in english\n",
    "full_word_list = full_word_list.head(50000)\n",
    "\n",
    "\n",
    "#Removing english stop words \n",
    "for word in full_word_list['Word']:\n",
    "    if word in stopwords.words('english'):\n",
    "        full_word_list = full_word_list[full_word_list.Word != word]\n",
    "        \n",
    "#Computing the percentage of occurence:\n",
    "full_word_list['Occurence_percentage'] = (full_word_list['Count']/ Word_count_total)*100\n",
    "#computing weight of words\n",
    "full_word_list['Weight']= 1/full_word_list['Occurence_percentage']\n",
    "\n",
    "display(full_word_list.shape)\n",
    "display(full_word_list.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing non english words\n",
    "Because they are much less commonly encountered in the dataset, words that are not in english will be ranked with a very high complexity. In addition to introduce a bias in the lexical complexity analysis, they wil also cause trouble when treating the polarity, because the VADER library is solely analysing english words. We will use the NLTK library in order to remove each non-english word from the bags of words.\n",
    "\n",
    "We first need to download the \"wordnet\" NLTK package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#Using the NLTK downloader to get wordnet\n",
    "#nltk.download()\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in full_word_list.index: \n",
    "    if not wn.synsets(full_word_list.Word[j]):#Comparing if word is non-English\n",
    "        full_word_list.drop(j, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_word_list = full_word_list.sort_values('Weight', ascending=False)\n",
    "display(full_word_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to get the complexity of one song by analyzing the weight of all his word\n",
    "def complexity_Song(lyrics):\n",
    "    #create a variable to stock the sum of the weights for every word of the song\n",
    "    sum_weight= 0\n",
    "    #split the lyrics to get an array of words and not just one big string\n",
    "    lyric = lyrics.split(' ')\n",
    "    #lyric = lyric.remove(' ')\n",
    "    \n",
    "    #filtering empty values\n",
    "    lyric = list(filter(None, lyric))\n",
    "    \n",
    "    #Removing every english stopword from the given lyric\n",
    "    lyric = [word for word in lyric if word not in stopwords.words('english')]\n",
    "    \n",
    "    for x in lyric:\n",
    "        #Making sure that the data is not empty\n",
    "        if len(full_word_list.loc[full_word_list['Word'] == x]['Weight'].values) != 0 :\n",
    "            sum_weight += full_word_list.loc[full_word_list['Word'] == x]['Weight'].values  \n",
    "\n",
    "    return float(sum_weight/len(lyric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment: \n",
    "This implementation is inspired by the TF-IDF algorithm. If the occurence of a word is weak in a dataset, it means that it is less common in the language, meaning that the lexical complexity is higher.\n",
    "\n",
    "English stopwords are very common in every sentences, they are used so typically that they do not add anything relevant to the analysis. This is the reason why we take them out. Our complexity analysis must be focused on words that do not appear regularly.\n",
    "### Analysis\n",
    "We need to go from word frequency to bags of words. Once this is done using our \"create_text\" function, we will use the polarity analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Resetting index\n",
    "matches.reset_index(inplace=True)\n",
    "\n",
    "#Intiating an empty column, in order to be able to interate on it\n",
    "matches['Bags_of_words'] = ''\n",
    "#Getting all the textual data in the DataFrame\n",
    "for i in matches.index:\n",
    "    matches.at[i, 'Bags_of_words'] = create_text(matches.at[i, 'words_freq'])\n",
    "\n",
    "#Because we have all the intial data in our DataFrame, we will store it as pickle object\n",
    "matches.to_pickle('full_table.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the bags of words in the DataFrame, we can conduct the analysis. Let us first work with the polarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#taking out the pickle object\n",
    "matches = pd.read_pickle('full_table.pkl')\n",
    "\n",
    "#Applying the polarity analysis for the bags of words\n",
    "for i in matches.index:\n",
    "    matches.at[i, 'Polarity_score'] = analyser.polarity_scores(matches.at[i, 'Bags_of_words'])['compound']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(matches.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting outputs in valuable categories\n",
    "Because we want a precise data structure, we must aggregate our outputs the most efficient way for later visualization. \n",
    "\n",
    "We need metadata per topic, per genre and per artist. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(color_codes=True, style=\"darkgrid\")\n",
    "\n",
    "def polarity_graph_generator(Data_in, categorization):\n",
    "    \n",
    "    Data_in[categorization] = Data_in[categorization].astype('category')\n",
    "\n",
    "    for cat in Data_in[categorization].cat.categories:\n",
    "        Division = pd.DataFrame()\n",
    "        Division = Data_in[(Data_in[categorization] == cat)]\n",
    "        \n",
    "        #Sorting values by polarity to create a graph\n",
    "        Division = Division.sort_values('Polarity_score', ascending=False)\n",
    "        #Index reseting\n",
    "        Division = Division.reset_index()\n",
    "        #plotting the results\n",
    "        sns_plot = sns.tsplot(Division['Polarity_score'], color='m').set_title('Polarity in {}'.format(cat))\n",
    "        x = len(Division['Polarity_score'])\n",
    "        y = Division['Polarity_score']\n",
    "        ax = sns_plot.axes\n",
    "        ax.fill_between(x, 0, y)\n",
    "        fig = sns_plot.get_figure()\n",
    "        #Storing the graph (MUST GREATE THE FOLDER BEFORE !)\n",
    "        fig.savefig(\"Polarity_plots/{} polarity.png\" .format(cat))\n",
    "        \n",
    "        #Clearing the figure\n",
    "        fig.clf()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running for every genre\n",
    "polarity_graph_generator(matches, 'genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Topic classification\n",
    "Having the data divided in genres in important for our analysis, however we are still missing one key dimension to make our work relevant for social good: The topic that is adressed in the songs. We must be able to know which subject is dealt with in a song, and then we will aggregate the data for the genre and we will be able to understand how a particular genre is handling a specific topic. For this part we are still considering two options:\n",
    "\n",
    "##### Option 1: Use the classifier from the 20newsgroup\n",
    "The work that was done in the second part of Homework 4. The good point is that we have a tuned algorithm to classify text into 20 different classes with more than 85% of accuracy. It is very powerful but not the exact algorithm we are looking for. It provides a strict classification, putting a text into one precise class while we would rather have several tags for a particular music. Artists tend to treat several topics we writing a song, and the 20newsgroup classifier would limitate our data treatment.\n",
    "\n",
    "Some data cleaning will be required if applying the 20newsgroup classifier. For instance categories dealing with computer science are likley to be irrelevant when classifying songs, thus why they will be taken away. \n",
    "\n",
    "##### Option 2: Find a deep learning algorithm/dataset that provides tags for songs\n",
    "We are still exploring this possibility because option 1 does not provide the most appealing data we want. We have not encountered yet an algorithm that would provide a categorization with several tags. We are aware that we may not find such classifier, but since the 20newsgroup one is already fully implemented and ready to use (Tf-idf and RandomForest are coded. We will look for another option for as long as we can.\n",
    "\n",
    "\n",
    "In the end, we opted for option 2\n",
    "For each class label, we used www.thesaurus.com to find synonyms and related words. In addition to this, we added words that seemed relevant to us.\n",
    "In order to have a strong classifier, we also used the extended dictionnary from google (COMPLETE WITH SOURCE AND EXACT NAME)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import global vectors from stanfords pretrained set, trained on tweets, one can choose wished dim=25,50,100,200\n",
    "global_vectors = HP.load_gensim_global_vectors(GS_200DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the words that define the chosen topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating words from wordembedded vectors\n",
    "gen_racism = HP.generate_related_words(\"racism\", global_vectors, topn=10)\n",
    "gen_women = HP.generate_related_words(\"women\", global_vectors, topn=10)\n",
    "gen_revolution = HP.generate_related_words(\"revolution\", global_vectors, topn=10)\n",
    "gen_money = HP.generate_related_words(\"racism\", global_vectors, topn=10)\n",
    "gen_politics = HP.generate_related_words(\"racism\", global_vectors, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the topics\n",
    "racism = ['racism', 'nigger', 'negro', 'race', 'racist', 'bigot', 'bigotry', 'apartheid', 'discrimination', 'segregation', 'unfairness', 'partiality', 'sectarianism', 'colored']\n",
    "women = ['women','girl', 'daughter', 'mother', 'she', 'wife', 'aunt', 'gentlewoman', 'girlfriend', 'grandmother', 'matron', 'niece', 'spouse', 'miss', 'genre']\n",
    "money = ['money','bill', 'capital', 'cash', 'check', 'fund', 'pay', 'payment', 'property', 'salary', 'wage', 'wealth', 'banknote', 'bankroll', 'bread', 'bucks', 'chips', 'coin', 'coinage', 'dough', 'finances', 'funds', 'gold', 'gravy', 'greenback', 'loot', 'pesos', 'ressources', 'riches', 'roll', 'silver', 'specie', 'treasure', 'wad', 'wherewithal']\n",
    "revolution = ['revolution','change', 'overthrow', 'demand', 'freedom', 'war', 'movement', 'brotherhood', 'reform', 'radical', 'leadership']\n",
    "politics =  ['politics','campaigning','government','backroom','civics','electioneering','legislature','policy','political']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create topic vectors so we can calculate a words similarity to topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lists so one can iterate through the topics\n",
    "name_of_topics = ['racism', 'women', 'money', 'revolution', 'politics']\n",
    "own_topics = [racism,women,money,revolution, politics]\n",
    "gen_topics = [gen_racism, gen_women, gen_money, gen_revolution, gen_politics]\n",
    "\n",
    "# loop through the topics, create the topic, and group same topics in the same tuple\n",
    "topic_vectors = []\n",
    "for own, gen in zip(own_topics, gen_topics):\n",
    "    gen_topic_vector = create_topic(gen, global_vectors)\n",
    "    own_topic_vector = create_topic(own, global_vectors)\n",
    "    topic_vectors.append((own_topic_vector, gen_topic_vector))\n",
    "\n",
    "    \n",
    "# example racism_ differnece of \"related words to topic\"\n",
    "print(\"Example of difference in \\\"related words\\\":\\n\\ngenerated words:\\n\", gen_women)\n",
    "print(\"Our picked words:\\n\", women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate every words relation to the different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vocab_topics = full_word_list.copy(deep=True)\n",
    "perc_dim_to_compare = 0.4\n",
    "\n",
    "\n",
    "for topics, topic_name in zip(topic_vectors, name_of_topics):\n",
    "    own_topic = topics[0]\n",
    "    gen_topic = topics[1]\n",
    "    print(topic_name)\n",
    "    \n",
    "    own_topic_scores = []\n",
    "    gen_topic_scores = []\n",
    "    for word in word_sample.Word:\n",
    "        own_topic_scores.append(calculate_topic_similarity(word, own_topic[0],\n",
    "                            global_vectors, std_dims=own_topic[1], perc_dim_to_compare=perc_dim_to_compare))\n",
    "        gen_topic_scores.append(calculate_topic_similarity(word, gen_topic[0],\n",
    "                            global_vectors, std_dims=gen_topic[1], perc_dim_to_compare=perc_dim_to_compare))\n",
    "    \n",
    "    # give a suitable cloumnname\n",
    "    own_columnname = \"own_topic_\" + topic_name\n",
    "    gen_columnname = \"gen_topic_\" + topic_name\n",
    "    vocab_topics[own_columnname] = pd.Series(own_topic_scores, index=word_sample.index)\n",
    "    vocab_topics[gen_columnname] = pd.Series(gen_topic_scores, index=word_sample.index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the dataframe and see the most relevant words for the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_vocab = vocab_topics.copy(deep=True)\n",
    "display(visual_vocab.sort_values('own_topic_revolution', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrapolate from (word <-> topic)-relation to (song <-> topic)-relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualization: How to meaningfully represent the data\n",
    "Now that we have the tools for a sentiment analysis, we must decide a way to visualize the results. Having a straightforward visualization will helop to compare the analyse features between genres, and this is precisely what we will try to achieve with the following visualization.\n",
    "\n",
    "All the interactive visualization structure that we have here will be displayed in a blog. \n",
    "\n",
    "## 3.1. Topic-based approach\n",
    "We first deal with polarity, then with lexical complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1.1. Dealing with polarity\n",
    "When having the topics and the polarity, we will be able to have an overview of how social topics are dealt with across music genres. We cannot draw any conclusion for this part without having every visual, but we believe that the comparison will provide a good insight on how music treats social topics.\n",
    "\n",
    "Once every visualization will be generated, here is the representation structure that we want to have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='Data/Organigram_polarity_viz.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Description\n",
    "- Each edge stands for a user's click.\n",
    "- Rectangles are representing the barplot of polarity (for each genre/artist at the level)\n",
    "- The user will pick a topic, this will display every genre treating the topic and their polarity barplot. Then when clicking on a genre's plot it will display what is polarity barplot for every artiss.\n",
    "\n",
    "\n",
    "### 3.1.2. Dealing with complexity\n",
    "We believe that there is a correlation between the complexity and the target audience. Has asserted earlier in this work, we do not have the sociological expertise to match the complextiy with precise social groups, but we assume that there is a correlation between the audience's age + educational level and the complexity of lyrics. \n",
    "\n",
    "We want to be able to visualize the complexity in two distinct ways. At first from a topic perspective by having genre displaying their complexity when treating a particular subject. Here is the display organigram we want to have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='Data/Organigram_complexity_viz.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description\n",
    "As before, edges stand for a user's click. Then when picking a genre, we will visualize what is the used lexical complexity. Comparing this value to this average of the genre will help to see if the target is more educated/older than the usual case for the genre.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The second visualization we want to provide is starting with topics on top. For every topic we want to be able to see the mean lexical complexity that is used by a genre to treat the topic. This will allow is to know what are the targeted audiences of each genres for a precise topic. Here is the data organigram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='Data/Organigram_complexity_topic_viz.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Data by location\n",
    "\n",
    "We want to know what topic is adressed in each genre, and especially how is it spread accross the globe. For that we will use the location data that we gathered. We want to produce maps for:\n",
    "- Complexity within a genre. In order to know how elaborate a genre is in different places around the world. We will use a heatmap to visualize the data.\n",
    "- Polarity of genres when dealing with a certain topic. For instance we want to be able to know if a subject has a negative polarity in rock in the US while it is positive in the UK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Chronological progression\n",
    "In order to know how a genre evolved, we want to plot the lexical complexity through time of every genre. We will use, again, a barplot. With the chronology on the x-axis an the complexity on the y-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finalization: Organization of the outputs\n",
    "\n",
    "\n",
    "## 4.1. \n",
    "\n",
    "##### Data handling\n",
    "Due to the fact that we are solely treating text, and that the lyrics are not entirely given, we use a volume of data that is relatively small. This situation enables us to have the original text files stored on our machines, and proceeding to storage of value when using the notebook is not necessary since our computer's cache is big enough to treat our data.\n",
    "\n",
    "##### Visualization\n",
    "We are convinced that the barplot is the most fitting representation for the generated graphs. We might find another librairy that makes it look more visually appealing, but we believe that it is the most meaningful way to show our results.\n",
    "\n",
    "##### Comparison\n",
    "Once we will have a topic-based split of our data, we will be able to compare how topics are treated in different genres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
