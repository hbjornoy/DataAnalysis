{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics - a GloVe implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO-list:\n",
    "- make visualization of decisionprocess\n",
    "- do topic for all words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrelated ideas\n",
    "- We *could* do PCA to vizualize glove technology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import gensim as gs\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import scipy\n",
    "\n",
    "# internal imports\n",
    "import helpers as HP\n",
    "\n",
    "\n",
    "\n",
    "# Constants: PS! put in your own paths to the files\n",
    "GLOVE_FOLDER = 'glove.twitter.27B'\n",
    "GS_FOLDER = 'gensim_glove_twitter_27B/'\n",
    "GS_25DIM = GS_FOLDER + \"gensim_glove_25dim.txt\"\n",
    "GS_50DIM = GS_FOLDER + \"gensim_glove_50dim.txt\"\n",
    "GS_100DIM = GS_FOLDER + \"gensim_glove_100dim.txt\"\n",
    "GS_200DIM = GS_FOLDER + \"gensim_glove_200dim.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim GloVe\n",
    "one can use gensims word2vec functions to check similarity and other interesting functions\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONLY ONE TIME: create the gensim_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spits out a .txt-file with the vectors in gensim format\n",
    "#HP.create_gensim_wv_from_glove(GLOVE_FOLDER)\n",
    "# afterwards you can delete the originalglovefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading global vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the created gensim-.txt file to create the word2vec so one can operate on it\n",
    "# choose wordvectors with the dimension you want\n",
    "global_vectors = HP.load_gensim_global_vectors(GS_50DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homophobia', 0.846714437007904),\n",
       " ('racial', 0.8199193477630615),\n",
       " ('sexism', 0.8197556734085083),\n",
       " ('misogyny', 0.8180930614471436),\n",
       " ('bigotry', 0.8014910221099854),\n",
       " ('discrimination', 0.7976340651512146),\n",
       " ('terrorism', 0.7949816584587097),\n",
       " ('racist', 0.7810259461402893),\n",
       " ('oppression', 0.7747340202331543),\n",
       " ('slavery', 0.7610248327255249)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_vectors.similar_by_word(\"racism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stone', 0.9959613680839539),\n",
       " ('queen', 0.9848849177360535),\n",
       " ('royal', 0.9840918779373169),\n",
       " ('prince', 0.9837151765823364),\n",
       " ('meets', 0.981272280216217),\n",
       " ('african', 0.9724677205085754),\n",
       " ('virgin', 0.9554038643836975),\n",
       " ('american', 0.9529185891151428),\n",
       " ('elizabeth', 0.9502763152122498),\n",
       " ('baker', 0.9487670660018921)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supposed to show queen, to demonstrate the power of word2vec, but failed #blameTwitterDataset\n",
    "global_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating related word to a topic\n",
    "related words should maybe be choosen by hand, if not one can use this method. Define how many words you want to define the topic with topn=__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homophobia', 'racial', 'sexism', 'misogyny', 'bigotry', 'discrimination', 'terrorism', 'racist', 'oppression', 'slavery', 'prejudice', 'hypocrisy']\n"
     ]
    }
   ],
   "source": [
    "related_words = HP.generate_related_words(\"racism\", global_vectors, topn=12)\n",
    "print(related_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating topic vector with associated standard deviations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "racism_topic_vector: [  5.3061676    4.14918756   2.2796092    4.47750092   3.64603496\n",
      "   8.85480309   0.98863995  -2.06385827   2.20178533   0.07488099\n",
      "   7.98649073  -9.27251053 -24.35789871   0.35689199   4.537889\n",
      "  -7.00892019  -2.06071401  -4.60118055  10.23355961   0.72619623\n",
      "  -1.88957298  -2.52386475   1.51116705   1.04792011   7.69085932\n",
      "  14.34574986   4.22350407   8.24262047  -8.73881149   1.84188223\n",
      "   5.30568695  -2.90989947  10.66682911  -8.85278893   1.79724383\n",
      "  -4.21171045  -9.31853962   3.79521108  -1.17262411 -16.17009926\n",
      "  10.79034042 -12.50316048   3.87884617  -9.67669106  -0.22767806\n",
      "   4.65888023   1.32117307   4.59731483  -0.98539394 -10.02946854]\n",
      "Standard deviation per dimension: [0.58725518, 0.29987451, 0.38871989, 0.37038887, 0.37891182, 0.3536672, 0.34381258, 0.26557761, 0.40413705, 0.29557183, 0.28638759, 0.27112234, 0.52783298, 0.58236593, 0.40554154, 0.42874056, 0.26720375, 0.45282122, 0.33696237, 0.41483811, 0.36824888, 0.38919759, 0.32197943, 0.33082637, 0.38160118, 0.2627351, 0.40555033, 0.38548547, 0.38243264, 0.3033452, 0.30239213, 0.39776051, 0.37284449, 0.45544329, 0.44413948, 0.49867257, 0.22631565, 0.3766335, 0.30173299, 0.49631995, 0.42169616, 0.38470757, 0.27274653, 0.4004961, 0.16909976, 0.36324149, 0.20500767, 0.38395232, 0.47460926, 0.4067806]\n"
     ]
    }
   ],
   "source": [
    "racism_topic_vector, racism_std_dims  = HP.create_topic(related_words, global_vectors)\n",
    "print(\"racism_topic_vector:\", racism_topic_vector)\n",
    "print(\"Standard deviation per dimension:\", racism_std_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating a words relation to topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niggers relation to racism 0.399370372295\n",
      "hope relation to racism 0.165352985263\n"
     ]
    }
   ],
   "source": [
    "# creating a wordvector\n",
    "word_vector = global_vectors['nigger']\n",
    "similarity_score = HP.calculate_topic_similarity(word_vector, racism_topic_vector, global_vectors, std_dims=False, perc_dim_to_compare=0.5)\n",
    "print(\"niggers relation to racism\", similarity_score)\n",
    "\n",
    "word_vector = global_vectors['car']\n",
    "similarity_score = HP.calculate_topic_similarity(word_vector, racism_topic_vector, global_vectors, std_dims=False, perc_dim_to_compare=0.5)\n",
    "print(\"hope relation to racism\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vizalization for decisionmaking of topics-words (+ for blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_related_words(related_words):\n",
    "    \"\"\" This function prints out the cosine distance between the wordvectors and the topicvector it creates from them.\n",
    "    It is meant as a tool to analyse (numerical) outliers of the words you pick to represent the group so you are sure\n",
    "    that you don't portrait the wrong meaning of the topic\n",
    "    \"\"\"\n",
    "    # look up the words in the vectorspace\n",
    "    word_vectors = [glove_model.wv[word] for word in related_words]\n",
    "    # sum up the wordvectors to one big vector representing the average \"meaning\" of all the words\n",
    "    topic_vector = sum(racism_vectors)\n",
    "    # Compute cosine distance from the individual wordvectors: To identify \n",
    "    cosine_dist_to_topic= [(scipy.spatial.distance.cosine(topic_vector, vec)) for vec in word_vectors]\n",
    "    print(\"Cosine distance from wordvector to topicvector:\\n\", dict(zip(related_words,cosine_dist_to_topic)))\n",
    "    \n",
    "    \"\"\" \n",
    "    TODO: plot overlapping polarcoordinates \n",
    "    \"\"\"\n",
    "\n",
    "analyze_related_words(racism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.path as path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "\n",
    "def polarplot_wordvector(wordvectors, yticks=None, ylim=None):\n",
    "    \n",
    "    # Choose some nice colors\n",
    "    matplotlib.rc('axes', facecolor = 'white')\n",
    "\n",
    "    # Make figure background the same colors as axes \n",
    "    fig = plt.figure(figsize=(10,8), facecolor='white')\n",
    "    \n",
    "    for i, wordvector in enumerate(wordvectors):\n",
    "        # Data to be represented\n",
    "        # ----------\n",
    "        properties = range(len(wordvector))\n",
    "        values = wordvector\n",
    "        # ----------\n",
    "\n",
    "        # Use a polar axes\n",
    "        axes = plt.subplot(111, polar=True)\n",
    "\n",
    "        # Set ticks to the number of properties (in radians)\n",
    "        t = np.arange(0,2*np.pi,2*np.pi/len(properties))\n",
    "        plt.xticks(t, [])\n",
    "\n",
    "        # Set yticks from 0 to 10\n",
    "        plt.yticks(np.linspace(np.min(wordvectors),np.max(wordvectors),5)) if yticks == None else plt.yticks(yticks)\n",
    "\n",
    "        # Draw polygon representing values\n",
    "        points = [(x,y) for x,y in zip(t,values)]\n",
    "        points.append(points[0])\n",
    "        points = np.array(points)\n",
    "        codes = [path.Path.MOVETO,] + \\\n",
    "                [path.Path.LINETO,]*(len(values) -1) + \\\n",
    "                [ path.Path.CLOSEPOLY ]\n",
    "        _path = path.Path(points, codes)\n",
    "        _patch = patches.PathPatch(_path, fill=False, color='blue', linewidth=0, alpha=.1)\n",
    "        axes.add_patch(_patch)\n",
    "        _patch = patches.PathPatch(_path, fill=False, linewidth = 2)\n",
    "        axes.add_patch(_patch)\n",
    "\n",
    "\n",
    "        # Set axes limits\n",
    "        plt.ylim(np.min(wordvectors),np.max(wordvectors)) if ylim == None else plt.ylim(ylim)\n",
    "\n",
    "        # Draw ytick labels to make sure they fit properly\n",
    "        for i in range(len(properties)):\n",
    "            angle_rad = i/float(len(properties))*2*np.pi\n",
    "            angle_deg = i/float(len(properties))*360\n",
    "            ha = \"right\"\n",
    "            if angle_rad < np.pi/2 or angle_rad > 3*np.pi/2: ha = \"left\"\n",
    "            plt.text(angle_rad, np.max(values), properties[i], size=14,\n",
    "                     horizontalalignment=ha, verticalalignment=\"center\")\n",
    "\n",
    "            # A variant on label orientation\n",
    "            #    plt.text(angle_rad, 11, properties[i], size=14,\n",
    "            #             rotation=angle_deg-90,\n",
    "            #             horizontalalignment='center', verticalalignment=\"center\")\n",
    "\n",
    "        # Done\n",
    "        plt.hold\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarplot_wordvector(wordvectors=racism_vectors[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure similarity differently, based on vagueness of \"topic\"\n",
    "ideas:\n",
    "- use standard deviation in cosine distance to tell something about spread of topic and use this to adjust \"similarity\"\n",
    "- Use a measure of deviation that takes into consideration\n",
    "\n",
    "\"std/remove disputed features within category\" similarity-algorithm:\n",
    "- get std per feature, get indexes of the (top 10% or above limit XX) and remove features from topic_vector\n",
    "- remove features with same index from word to compare similarity with\n",
    "- do cosine similarity on those features. Have a limit,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
